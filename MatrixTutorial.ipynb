{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def printmat(M):\n",
    "    for i in range(len(M)):\n",
    "        print(M[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stefaans Grote Matrix-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Het concept\n",
    "\n",
    "##  Intro\n",
    "\n",
    "Er zijn veel manieren om matrices te introduceren. We volgen de meest intuitieve: onze matrices zijn lineare afbeeldingen tussen reele eindigdimensionele vectorruimtes. Say what?\n",
    "\n",
    "We zullen de introductie van matrices daarom doen in 2 stappen: we beschrijven eerst de vectorruimten waartussen ze werken, en dan de matrix-afbeeldingen zelf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorspace\n",
    "\n",
    "Een reele n-dimensionele vectorruimte is een moeilijke naam voor de verzameling van n-tupels met reele getallen als componenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=3\n",
    "vectorA = np.array( [1,1.3,3.1415926] )\n",
    "vectorB = np.array( [2,1.7,3.1415926] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "met als eigenschap dat we tupels kunnen optellen en vermenigvuldigen met reele getallen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorC = vectorA * 1.2 + vectorB * 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec v_C$ is een $lineaire\\ combinatie$ van $\\vec v_A$ en $\\vec v_B$ en daarom ook een $n-D$ vector. So far so good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineaire afbeeldingen\n",
    "\n",
    "Een lineare afbeelding $F$ tussen vectorruimten maakt van elke n-dimensionele vector een m-dimensionale vector, bijvoorbeeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 2\n",
    "def F(v):\n",
    "    return v[:2]\n",
    "wectorA = F(vectorA)\n",
    "wectorB = F(vectorB)\n",
    "wectorC = F(vectorC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zodat het beeld van lineare combinaties de lineaire combinatie van de beelden is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wectorC = wectorA * 1.2 + wectorB * 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kan nagaan dat dit effectief klopt voor $F$. Natuurlijk zijn er veel, veel meer van dergelijke functies. Volgens onze definitie worden ze allemaal voorgesteld door matrices, en ook omgekeerd geeft elke matrix zo'n functie. Why's that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e0 = np.array( [1,0,0] )\n",
    "e1 = np.array( [0,1,0] )\n",
    "e2 = np.array( [0,0,1] )\n",
    "\n",
    "vectorA = vectorA[0] * e0 + vectorA[1] * e1 + vectorA[2] * e2\n",
    "wectorA = vectorA[0] * F(e0) + vectorA[1] * F(e1) + vectorA[2] * F(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet hierboven dat we $\\vec w_A$ gevonden hebben door slechts 3 beelden te berekenen: $F(\\vec e_1)$, $F(\\vec e_2)$ en $F(\\vec e_3)$. Met diezelfde 3 wordt $elk$ vectorbeeld gevonden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wectorB = vectorB[0] * F(e0) + vectorB[1] * F(e1) + vectorB[2] * F(e2)\n",
    "wectorC = vectorC[0] * F(e0) + vectorC[1] * F(e1) + vectorC[2] * F(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F$ wordt dus volledig bepaald door de volgende gegevens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[0 1]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "MT = np.array( [F(e0), F(e1), F(e2)] )\n",
    "printmat(MT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waar eerst alle beelden $\\vec w_i$ uit $\\vec v_i$ werden gehaald met $F$, gebeurt dat nu met de getallen in $M_t$, die we een matrix noemen. Zoals gezegd, de matrix $M^T$ is equivalent aan een lineaire afbeelding $F$ tussen eindig dimensionele reele vectorruimten. Dat gaat op voor elke lineaire afbeelding $F$!\n",
    "\n",
    "De manier waarop we $M^T$ gebruiken is hierboven al aangegeven: elk beeld is een lineaire combinatie van de basis-beeldvectoren ${\\vec F} (\\vec e_0)$, $\\vec F (\\vec e_1)$ en $\\vec F (\\vec e_2)$\n",
    "$$ \\vec w_A = v_A[0]\\,\\vec F (\\vec e_0)+v_A[1]\\,\\vec F (\\vec e_1)+v_A[2]\\,\\vec F (\\vec e_2)$$\n",
    "en zo verder voor alle $\\vec w_i$'s. \n",
    "\n",
    "De standaard schrijfwijze voor zo'n lineaire combinatie --remember: niets meer dan een som van vectoren met elk eventueel een extra factor-- is als volgt:\n",
    "$$\\vec w_A = \\left[v_A[0]\\,,v_A[1]\\,,v_A[2]\\right]\\ \\cdot\\ \\left[\\begin{array}{c}\\vec F (\\vec e_0)\\\\\\vec F (\\vec e_1)\\\\\\vec F (\\vec e_2)\\end{array}\\right]=\\vec v_A\\ \\cdot\\ M^T$$\n",
    "en analoog voor andere vectoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wectorA = np.dot( vectorA,MT )\n",
    "wectorB = np.dot( vectorB,MT )\n",
    "wectorC = np.dot( vectorC,MT )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hebben dus het matrix-dotproduct ingevoerd, in dit geval om een vector met een matrix te vermenigvuldigen. Het resultaat is de som van alle rijen van de matrix met de vectorcomponenten als coefficienten\n",
    "$$ \\vec w_A = v_A[0]\\,\\vec F (\\vec e_0)+v_A[1]\\,\\vec F (\\vec e_1)+v_A[2]\\,\\vec F (\\vec e_2)$$\n",
    "zoals we al hadden gezien.\n",
    "\n",
    "Denk eraan dat de uitdrukking hierboven een gelijkheid is tussen 2-componentsvectoren. We zouden het resultaat ook kunnen schrijven voor beide componenten:\n",
    "$$ \\vec w_A[0] = v_A[0]\\, F (\\vec e_0)[0]+v_A[1]\\, F (\\vec e_1)[0]+v_A[2]\\, F (\\vec e_2)[0]$$\n",
    "en\n",
    "$$ \\vec w_A[1] = v_A[0]\\, F (\\vec e_0)[1]+v_A[1]\\, F (\\vec e_1)[1]+v_A[2]\\, F (\\vec e_2)[1]$$\n",
    "\n",
    "We gaan eens op een zotte quest en gaan 2 nieuwe vectoren maken uit de componenten van de $\\vec F(\\vec e_i)$'s of --equivalent-- uit de componenten van $M^T$.\n",
    "$$\\vec M_0=[ F (\\vec e_0)[0]\\,, F (\\vec e_1)[0]\\,, F (\\vec e_2)[0]]$$\n",
    "en\n",
    "$$\\vec M_1=[ F (\\vec e_0)[1]\\,, F (\\vec e_1)[1]\\,, F (\\vec e_2)[1]]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M0 = np.array( [ F(e0)[0],F(e1)[0],F(e2)[0] ] )\n",
    "M1 = np.array( [ F(e0)[1],F(e1)[1],F(e2)[1] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met minimale Math-Sherlock-skills vinden we nu 2 connecties: Ten eerst zijn de vectoren hierboven niets anders dan de kolommen van $M^T$, as you can check. \n",
    "$$M^T=[\\vec M_0^T,\\vec M_1^T]$$\n",
    "Ten tweede worden de componenten die we hierboven hadden afgeleid, eenvoudig geschreven als\n",
    "$$\\vec w_A[0] = v_A[0]\\,M_0[0]+v_A[1]\\,M_0[1]+v_A[2]\\,M_0[2]=\\vec v_A\\cdot\\vec M_0$$\n",
    "$$\\vec w_A[1] = v_A[0]\\,M_1[0]+v_A[1]\\,M_1[1]+v_A[2]\\,M_1[2]=\\vec v_A\\cdot\\vec M_1$$\n",
    "of korter:\n",
    "$$\\vec w_A = [ \\vec v_A\\cdot\\vec M_0\\,, \\vec v_A\\cdot\\vec M_1]$$\n",
    "\n",
    "Alweer twee conclusies: Ten eerste heeft onze quest ons een nieuwe schijfwijze opgeleverd voor het beeld $\\vec w_A=\\vec F(\\vec v_A)$, nice! Dat beindigt in feite de inleiding tot matrices als lineaire afbeeldingen. We zijn klaar voor wat technische stuff. Ten tweede... wait, what? ... een dot-product tussen vectoren?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dots, dots, dots...\n",
    "\n",
    "Het is tijd om 'the talk' te hebben. Technical, notational mumbo-jumbo waar we door moeten. \n",
    "\n",
    "### Vector dot\n",
    "\n",
    "We hebben in feite enkel twee manieren gezien om een matrix --die een lineaire afbeelding voorstelt-- te dotten met een vector, wat ultiem hetzelfde was als de originele afbeelding zelf uitvoeren. Methode 1 zei dat het beeld een combinatie van de matrix-rijen was, met de componenten van het origineel als coefficient. Methode 2 zei dat de componenten van het beeld elk een (ander) dot product waren van het origineel met een andere vector, de respectievelijke kolommen van de matrix, as we checked! (Or did you?)\n",
    "\n",
    "Dat 'ander' dot-product is het bekende dot-product (ook wel $in-product$ of $scalair-product$ of $contractie$) van vectoren van gelijke lengte en de som van producten van overeenkomstige coefficienten.\n",
    "\n",
    "$$\\vec v_A\\cdot\\vec v_B = v_A[0]v_B[0]+v_A[1]v_B[1]+v_A[2]v_B[2]$$\n",
    "\n",
    "We gaan direct in op het verband tussen beide dot-producten. Ze zijn -natuurlijk- intrinsiek hetzelfde, maar enkel notationeel licht verschillend.\n",
    "\n",
    "### Duale dots\n",
    "\n",
    "Een van de matrix-eigenschappen die we vooropgesteld hebben is dat ze en afbeelding zijn van een $n$-D naar een $m$-D vectorruimte. Dat wordt ook duidelijk uit de structuur van de matrix $M^T$ die $n$ rijen heeft en $m$ kolommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[0 1]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "printmat(MT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een speciaal geval is $m=1$. De matrix (say $N^T$) bestaat dan uit een enkele kolom-vector,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.]\n",
      "[ 1.7]\n",
      "[ 3.1415926]\n"
     ]
    }
   ],
   "source": [
    "NT = np.array( [[2],[1.7],[3.1415926]] )\n",
    "printmat(NT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " en de afbeelding of dus ons matrix-product genereert een $1-D$ vector aka een getal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14.07960406]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot( vectorA,NT ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hebben niet toevallig de componenten in $N^T$ gelijk gekozen aan die in $\\vec v_B$. Het laat ons toe het bovenstaande matrix-dot-product te vergelijken met een vector-dot-product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0796040644\n"
     ]
    }
   ],
   "source": [
    "print(np.dot( vectorA,vectorB ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, same thing. Het standaard vector-dot-product is niets anders dan het matrix-dot-product met een 1-kolom matrix aka $getransponeerde$ vector. \n",
    "\n",
    "Dat is een heel standaard ding: rij-vectoren, kolom-vectoren, matricesmet slechts 1 rij of kolom zijn allemaal volledig equivalent aan elkaar (de algebraische term is $isomorf$).\n",
    "\n",
    "Dat zorgt ervoor dat in veel codeertalen die dingen door elkaar worden gebruikt en bijvoorbeeld hetzelfde matrix-dot-product voor vectoren werkt (zoals hier in numpy). Word of caution, niet altijd wordt die sloppyness aanvaard, bijvoorbeeld niet in Matlab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of things\n",
    "\n",
    "Een gevolg van de verwisselbaarheid van rij en kolomvectoren is een soort mixed notation. Bear with me, dit is belangrijk. Meestal worden vectoren waarop een matrix inwerkt formeel als kolomvectoren geschreven. In code kunnen we de eenvoudiger rij-notatie gebruiken omdat het verschil -zoals gezegd- toch genegeerd wordt. Maar (!) het betekent dat de standaard volgorde bij de notatie van matrix-afbeeldingen omgekeerd is aan wat we tot hiertoe uit onze rij-vector notatie hebben afgeleid:\n",
    "$$ \\vec w_A = \\vec v_A\\cdot M^T\\quad\\Leftrightarrow \\vec w_A^T = (M^T)^T\\cdot\\vec v_A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De $T$ (transpose) switcht rij-en kolomvectoren en ook de overeenkomstige matrices. Twee keer switchen is natuurlijk hetzelfde als niets doen dus\n",
    "$$(M^T)^T=M$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n",
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "M = np.transpose(MT)\n",
    "M = MT.T\n",
    "printmat(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zo zien we hierboven dat inderdaad de matrix $M$ langs links inwerkt op de kolomvecotr $v_A^T$ terwijl voordien $M^T$ langs rechts op de rijvector $v_A$ inwerkte. Je hoeft het niet te lang te onthouden, het wordt compleet evident in de volgende sectie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   1.3]\n",
      "[ 1.   1.3]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot( MT.T,vectorA ))\n",
    "print(np.dot( vectorA,MT ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze nieuwe notatie wordt alles omgekeerd: We konden voordien de rijvector $\\vec w_A$ schrijven als hetzij een som van rijvectoren met de componenten $v_A[0]$m $v_A[1]$ en $v_A[2]$ als coefficienten:\n",
    "$$ \\vec w_A = v_A[0]\\,\\vec F (\\vec e_0)+v_A[1]\\,\\vec F (\\vec e_1)+v_A[2]\\,\\vec F (\\vec e_2)$$\n",
    "of -equivalent- als een rij-vector met vector dot-product componenten:\n",
    "$$\\vec w_A = [ \\vec v_A\\cdot\\vec M_0\\,, \\vec v_A\\cdot\\vec M_1]$$\n",
    "Check once again: same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   1.3]\n",
      "[1.0, 1.3]\n"
     ]
    }
   ],
   "source": [
    "print( vectorA[0]*F(e0)+vectorA[1]*F(e1)+vectorA[2]*F(e2)) \n",
    "print( [np.dot(vectorA,M0),np.dot(vectorA,M1)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omdat we nu een kolom-vector moeten uitkomen, zijn de 3 2-D vectoren $\\vec F(e_i)$ nu kolommen en de overeenkomstige matrix van de afbeelding heeft 2 rijen en 3 kolommen, ipv omgekeerd zoals voorheen. De conventie voor rij-vectoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[0 1]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "wectorA = np.dot( vectorA,MT )\n",
    "printmat(MT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordt nu dus vervangen door de kolom-vector notatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n",
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "wectorA = np.dot( M,vectorA )\n",
    "printmat(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "die een matrix met dezelfde informatie (want het is dezelfde afbeelding $F$) in een andere orde gebruikt. Vanaf nu werken transformatie-matrices in op vectoren langs de linkerkant, wat standaardnotatie is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains of dots\n",
    "\n",
    "Nu we alle mogelijke inwerking van matrices op vectoren begrijpen zijn we klaar om matrices met matrices te combineren. We zijn vertrokken met het idee dat matrix $M$ $n$-D vectoren omzet in $m$-D vectoren. Wat als we nu een andere matrix $K$ hebben die $m$-D vectoren omzet in $k$-D vectoren:\n",
    "$$\\vec w_A = M\\cdot \\vec v_A$$\n",
    "$$\\vec u_A = K\\cdot\\vec w_A$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "[1 0 0]\n",
      "[0 1 0]\n",
      "K\n",
      "[0 1]\n",
      "[2 3]\n",
      "[4 5]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "K = np.arange(k*m).reshape(k,m)\n",
    "print('M')\n",
    "printmat(M)\n",
    "print('K')\n",
    "printmat(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wat is dan de matrix die $\\vec v_A$ omzet in $\\vec u_A$? \n",
    "$$\\vec u_A = K\\cdot M\\cdot\\vec v_A\\stackrel{?}{=}L\\cdot \\vec v_A$$\n",
    "Blijkt dat $L$ bestaat en --niet onverwacht-- de combinatie van beelden van $M$-kolommen onder de linkse inwerking van $K$ of beelden van $K$-rijen onder de rechtse inwerking van $M$.\n",
    "$$L = \\left[\\begin{array}\\vec K[0]\\cdot M\\\\\\vec K[1]\\cdot M\\\\\\vec K[2]\\cdot M\\\\\\vec K[3]\\cdot M\\end{array}\\right]$$\n",
    "\n",
    "of\n",
    "$$L = [\\ K\\cdot \\vec M[:,0]\\ ,\\ K\\cdot \\vec M[:,1]\\ ,\\ K\\cdot \\vec M[:,2]\\ ]$$\n",
    "Twee equivalente berekeningswijzen die op hun beurt niets anders zeggen dan een 3e berekeningswijze:\n",
    "\n",
    "\"De $(i,j)$e component van $K\\cdot M$ is het vector-dot-product van de $i$e rij van $K$ met de $j$e kolom van $M$.\"\n",
    "\n",
    "en dit is de formele definitie van het matrixproduct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n",
      "[0 1 0]\n",
      "[2 3 0]\n",
      "[4 5 0]\n",
      "[6 7 0]\n"
     ]
    }
   ],
   "source": [
    "L = np.dot( K,M )\n",
    "#1e berekening\n",
    "L = np.array( [np.dot( K[i],M ) for i in range(k)] )\n",
    "#2e berekening\n",
    "L = np.array( [np.dot( K,M[:,j] ) for j in range(n)]).T\n",
    "#3e berekening\n",
    "L = np.array( [[np.dot( K[i],M[:,j] ) for j in range(n)] for i in range(k)] )\n",
    "print('L')\n",
    "printmat(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om matices te vermenigvuldigen moet dus het aantal kolommen (dimensie van de rij-vectoren) van de linkse gelijk zijn aan het aantal rijen(dimensie van de kolom-vectoren) van de rechtse. Om snel zo'n product uit te voeren nemen we denkbeeldig een ($i$e) rijvector van de linkermatrix, een geroteerde ($j$e) kolomvector van de rechter en leggen die over elkaar zodat we ze kunnen 'dotten'. Dat geeft ons een getal dat we in de overeenkomstige $(i,j)$e positie schrijven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecties en vierkante Matrices\n",
    "\n",
    "Om de inleiding te besluiten moeten we nog 2 belangrijke klasses van matrices bespreken. \n",
    "\n",
    "### Projecties\n",
    "\n",
    "Het kan gebeuren dat de 'uitgaande' kolomdimensie van een matrix, die in principe de dimensie vna de beelden bepaald, niet de echte dimensie van beeld is. Een voorbeeld is de projectie $P$ van een willekeurige $2D$ vector op een rechte, pak de eerste bissectrice $x=y$. De projectie rekt uit met het origineel, en een projectie van een som van vectoren is ook de som van projecties, alle voorwaarden voor een lineaire afbeelding en dus is er een matrixvoorstelling van de afbeelding. Hoewel de beelden een $x$- en $y$-component hebben, is het resultaat toch $1$-dimensionaal. \n",
    "\n",
    "Let's see. Om de projectie te maken hebben we een eenheidsvector nodig langs de $x=y$ lijn:\n",
    "$$\\vec e = [\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}]$$\n",
    "He is evident dat inderdaad $x=y$ en snel gechecked dat het een eenheidsvector is. De lengte van de projectie wordt nu gegeven door het dot-product:\n",
    "$$P(\\vec v)=\\vec e\\,(\\vec e\\cdot\\vec v)$$\n",
    "een uitdrukking die we als matrixproduct kunnen schrijven, het is snel gechecked dat de matrix\n",
    "$$P = \\left[\\begin{array}{cc}e[0]e[0]&e[0]e[1]\\\\e[1]e[0]&e[1]e[1]\\end{array}\\right]=\\left[\\begin{array}{cc}0.5&0.5\\\\0.5&0.5\\end{array}\\right]$$\n",
    "het voorgaande resultaat geeft. (We misbruiken vanaf nu wat notatie waarbij we dezelfde letter gebruiken voor matrix en afbeelding, anders wordt het alfabet snel te klein). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.15  1.15]\n",
      "[ 1.15  1.15]\n"
     ]
    }
   ],
   "source": [
    "wectorE = np.ones((2,)) / np.sqrt(2)\n",
    "print(wectorE * np.dot(wectorE,wectorA))\n",
    "print(np.dot( 0.5 * np.ones((2,2)),wectorA ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alweer vereist het geen echte Sherlock-skills om in te zien dat het beeld van een willekeurige vector onder P $x$ en $y$ component gelijk zal hebben, zoals bedoeld. Dat betekent omgekeerd dat een heel deel vectoren (die niet op de eerste bissectrice) nooit bereikt worden met een projectie. De eigenlijke dimensie van het beeld is dus 1, terwijl we er toch 2 componentsvectoren uit krijgen. De eigenlijke dimensie van het beeld wordt de $rang$ genoemd. \n",
    "\n",
    "### Vierkante matrices\n",
    "\n",
    "Het veruit meest interessante toepassingsdomein van matrices is het geval waarin ze afbeeldingen van en naar vectorruimten met dezelfde dimensie voorstellen. In dat geval hebben ze evenveel rijen als kolommen en worden ze \"vierkante matrices\" genoemd. De reden achter die naamgeving is zo complex... you would not understand.\n",
    "\n",
    "Een eerste extraatje bij vierkante matrices is dat ze oneindig dot-baar zijn, de restrictie dat er evenveel kolommen links als rijen rechts zijn in een matrixproduct vervalt natuurlijk als alle matrices evenveel rijen als kolommen hebben. \n",
    "\n",
    "Een 2e extraatje is dat ze soms inverteerbaar zijn. Niet transponeerbaar -dat zijn alle matrices- maar inverteerbaar. Het betekent dat elk origineel met juist een beeld correspondeert en we de omgekeerde bewerking kunnen maken. Het is duidelijk dat voor niet-vierkante matrices, als de dimensie van de beeld-ruimte verschilt van die van het origineel, zoiets onmogelijk is. Maar zelfs in het vierkante geval is inverteerbaarheid niet evident. Precies in het geval van projecties dat we net hebben besproken is dat niet mogelijk. Of anders gezegd: Een $n\\times n$ matrix is inverteerbaar als ook de rang gelijk is aan $n$.\n",
    "\n",
    "Is die inverse dan de getransponeerde? In het algemeen niet, maar soms wel. We zullen extra aandacht besteden aan de klasse van matrices waarvoor dit geldt -isometrieen- maar zeggen nog eens: zeker niet in het algemeen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Application time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan 3 soorten toepassingen bespreken. Matrices kunnen worden gebruikt om stelsels op te lossen in bijvoorbeeld vraagstukken, ze hebben een aantal statistische toepassingen zoals in Markov chains, lineaire regressie of zelfs neurale netwerken en tenslotte de klasse van isometrieen bij geometrische toepassingen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineaire Algebra\n",
    "\n",
    "\"Ik zie 10 koppen, 20 handen en 12 benen, hoeveel paarden, apen en kippen zijn er\" Is een klassiek vraagstuk. De vertaling naar een algebraisch stelsel gaat als volgt:\n",
    "$$\\begin{array}{rcl}10&=&p+a+m\\\\20 &=& 4\\,a\\\\12&=&4\\,p+2\\,k\\end{array}$$\n",
    "Wat met alle matrix-machinerie die we hebben ook kan geschreven worden als \n",
    "$$\\left[\\begin{array}{c}10\\\\20\\\\12\\end{array}\\right]=\\left[\\begin{array}{ccc}1&1&1\\\\0&4&2\\\\4&0&2\\end{array}\\right]\\cdot\\left[\\begin{array}{c}p\\\\a\\\\k\\end{array}\\right]$$\n",
    "om dit stelsel te kunnen oplossen moeten we de matrix inverteren, dat wil zeggen een matrix $A^{-1}$ vinden zodat\n",
    "$$A^{-1}\\cdot A=Id$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([[1,1,1],[4,0,0],[0,4,2]])\n",
    "telling = np.array([10,20,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om dat te kunnen moeten we eerst bepalen of de matrix $A$ geen projectie is. In dat geval is inverteren niet mogelijk, zoals we al hebben besproken. We hebben dan zowel beelden die geen origineel hebben als beelden die er veel hebben. Om te begrijpen wanneer en hoe we de inversie kunenn doen moeten we eerst een eenvoudiger probleem leren oplossen, een diagonaalmatrix inverteren.\n",
    "\n",
    "### Diagonaalmatrix\n",
    "\n",
    "Had ik in het vorige vraagstuk over mensen gehad en gevraagd naar het aantal neuzen, vingers en tenen, dan zag het stelsel er als volgt uit:\n",
    "$$\\left[\\begin{array}{c}10\\\\20\\\\12\\end{array}\\right]=\\left[\\begin{array}{ccc}1&0&0\\\\0&5&0\\\\0&0&5\\end{array}\\right]\\cdot\\left[\\begin{array}{c}n\\\\v\\\\t\\end{array}\\right]$$\n",
    "De matrix in de vergelijking is een diagonaalmatrix en gemakkelijk te inverteren: de inverse is eenvoudig die diagonaalmatrix van de inversen:\n",
    "$$\\left[\\begin{array}{ccc}1&0&0\\\\0&0.2&0\\\\0&0&0.2\\end{array}\\right]\\cdot\\left[\\begin{array}{c}10\\\\20\\\\12\\end{array}\\right]=\\left[\\begin{array}{c}n\\\\v\\\\t\\end{array}\\right]$$\n",
    "en het stelsel is snel opgelost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse\n",
      "[ 1.  0.  0.]\n",
      "[ 0.   0.2  0. ]\n",
      "[ 0.   0.   0.2]\n",
      "result\n",
      "[ 10.    4.    2.4]\n"
     ]
    }
   ],
   "source": [
    "B =np.diag([1,5,5])\n",
    "print('inverse')\n",
    "printmat(np.linalg.inv(B))\n",
    "print('result')\n",
    "print(np.dot(np.linalg.inv(B),telling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De voorwaarde voor inverteerbaarheid is hierbij duidelijk: geen van de diagonaalelementen van de matrix mocht $0$ zijn, en dus mag hun product niet nul zijn. Dat product heet de $determinant$ van de diagonaalmatrix. Het blijkt dat met bijna alle matrices een diagonaalmatrix overeenkomt, zodanig dat de inverse afhangt van de inverse van die diagonaalmatrix. Voor een willekeurige matrix is het product van de diagonaalelementen van de corresponderende diagonaalmatrix ook zijn determinant. De diagonaalelementen zelf worden eigenwaarden genoemd.\n",
    "\n",
    "Als de determinant 0 is, en de matrix heeft minstens een eigenwaarde die 0 word, dan is de rang niet meer maximaal en is de matrix een (mogelijk geschaalde) projectie. Het betekent dat een aantal componenten van het beeld in een vaste combinatie moeten voorkomen, zoals in ons projectievoorbeeld $x=y$. Voor al die combinaties zijn er oneindig veel originelen, en voor de andere zijn er geen. Dit is meestal niet ideaal om een stelsel op te lossen.\n",
    "\n",
    "Is de determinant verschillend van 0 is het stelsel wel oplosbaar. Dan kunnen we alle eigenwaarden en daardoor ook de matrix zelf inverteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse\n",
      "[ 0.    0.25  0.  ]\n",
      "[-1.    0.25  0.5 ]\n",
      "[ 2.  -0.5 -0.5]\n",
      "result\n",
      "[ 5.  1.  4.]\n"
     ]
    }
   ],
   "source": [
    "print('inverse')\n",
    "printmat(np.linalg.inv(A))\n",
    "print('result')\n",
    "print(np.dot(np.linalg.inv(A),telling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De juiste berekening van eigenwaarden, determinanten en inversen zullen we op het einde van de tutorial nog eens uit de doeken doen. Dat is een beetje technisch.\n",
    "\n",
    "## Statistiek\n",
    "\n",
    "De meeste toepassingen van matrices gaan over vectorruimtes met een zekere symmetrie tussen de dimensies: lengte, hoogte en breedte zijn gelijkwaardige grootheden. Maar dat hoeft niet zo te zijn. \n",
    "\n",
    "Stel bijvoorbeeld dat we de afwijking (langs ($a_\\parallel$ en loodrecht $a_\\perp$) op de as startpositie - put) van een golfbals finale positie na een slag tov de afstand voor de slag ($\\Delta$), de ranking van de speler($R$) en de leeftijd van de speler ($L$). We meten dat een heel jaar lang en vinden bijvoorbeeld honderd verbanden \n",
    "$$(\\Delta,R,L) \\mapsto (a_\\parallel,a_\\perp)\\,.$$\n",
    "\n",
    "We gaan er vanuit dat er een eenoudig universeel $lineair$ recept is om dat verband te implementeren. Een lineaire afbeelding tussen een 3-vector en een 2-vector is volgens onze beschouwingen een matrix, dus we zoeken een verband:\n",
    "$$\\left[\\begin{array}{c}a_i^\\parallel\\\\a_i^\\perp\\end{array}\\right]=W\\cdot \\left[\\begin{array}{c}\\Delta_i\\\\R_i\\\\L_i\\end{array}\\right]$$ voor alle 100 $i$'s,\n",
    "met natuurlijk $W$ een $2\\times 3$ matrix. \n",
    "\n",
    "Het vinden van zo'n linear verband tussen variabelen heet lineaire regressie en naast de matrix $W$ zoals hirboven hangt die regressie in het algemeen nog een constante bias-vector $B$, die voor onze tutorial over matrices niet echt een rol speelt. \n",
    "\n",
    "Twee opmerkingen zijn belangrijk. Ten eerste is de bovenstaande vergelijking een voorbeed van een neuraal netwerk. Hoewel zo'n netwerk in het algemeen mer lagen en niet-lineariteit bevat, bevat een eenvoudig lineaire regressie al enkele key-elements: De matrix $W$ bestaat uit 6 gewichten die de 3 input nodes (componenten van onze input-vector) verbinden met de 2 output-nodes (componenten van de output vector). Voor het bepalen van de optimale $W$ bestaan in dit geval wel exacte methdoden in plaats van een iteratieve inferentie zoals bij neurale netwerken.\n",
    "\n",
    "Ten tweede kunnen we het belang van de matrix-rang hier aantonen. De $2\\times 3 $ matrix $W$ heeft maximaal rang 2, wat betekent dat maar 2 lineaire combinaties van $\\Delta$, $R$ en $L$ onafhankelijk zullen bijdragen tot $a_\\parallel$ en $a_\\perp$. Die combinaties worden pricipaalcomponenten genoemd. In het geval van vierkante matrices heten ze ook eigenvectoren, terwijl in dat geval de corresponderende gewichten eigenwaarden zijn. Remember: Eigenwaarden zijn elementen van een diagonaalmatrix die geassocieerd is aan een vierkante matrix, en het bepalen ervan is hetzelfde proces als het bepalen van de principaalcomponenten. Dit is de tweede keer dat we eigenwaarden op een vage manier definieren, later gaan we er dieper op in.\n",
    "\n",
    "### Geometrie\n",
    "\n",
    "De meest voor de hand liggende toepassing van lineaire afbeeldingen zijn isomorfismes van de ruimte. We gaan enkel naar 2D en 3D voorbeelden kijken. Met isomorfisme bedoelen we hier dat we een afbeelding toepassen van de ruimte op zichzelf zodat afstanden en grootte van hoeken hetzelfde blijven. Isomorfismes zijn rotaties, spiegelingen en verschuivingen.\n",
    "\n",
    "Rotaties en spiegelingen zijn lineaire afbeeldingen en kunnen dus als matrix voorgesteld worden. Die matrices hebben de bepalende eigenschap dat hun inverse gelijk is aan hun getransponeerde. Dat is niet moeilijk in te zien. Aangezien afstanden en groottes van hoeken precies bepaald worden door het dot-product, zoeken we de matrices die alle dot-producten onveranderd laten. \n",
    "$$(O\\cdot \\vec v)\\cdot (O\\cdot \\vec v) = \\vec v \\cdot\\vec v$$\n",
    "We hebben ook al gezien dat het vector-dot-product eigenlijk een matrix-product is tussen een vector en een getransponeerde vector, en dat een links product van een matrix met een kolomvector een rechts product van de getransponeerde. Dus\n",
    "$$ (O\\cdot \\vec v)\\cdot (O\\cdot \\vec v) = \\vec v\\cdot O^T\\cdot O\\cdot\\vec v$$\n",
    "en het is duidelijk dat $$O^T=O^{-1} \\Rightarrow O\\cdot O^T=1$$\n",
    "hiervoor voldoende is. Het blijkt dat als dit waar moet zijn voor alle vectoren $\\vec v$ dit ook de enige mogelijkheid is\n",
    "\n",
    "#### 2D transformaties\n",
    "\n",
    "Alle orthogonale transformaties in 2D zijn van de vorm\n",
    "$$O_{rot}\\,\\quad O_{sp}$$\n",
    "of een combinatie van dergelijke matrices. \n",
    "\n",
    "De matrixvoorstelling van een spiegeling is bijvoorbeeld:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1]\n",
      "[ 2 -1]\n",
      "[ 1.  2.]\n"
     ]
    }
   ],
   "source": [
    "Osp1 = np.diag([1,-1])\n",
    "a = np.pi / 4\n",
    "Osp2 = np.array([[np.cos(2*a),np.sin(2*a)],[np.sin(2*a),-np.cos(2*a)]])\n",
    "\n",
    "v= np.array([2,1])\n",
    "print(v)\n",
    "print(np.dot(Osp1,v))\n",
    "print(np.dot(Osp2,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "waarbij $O_{sp,1}$ een spiegeling rond de $X-as$ en $O_{sp,2}$ een spiegeling rond de eerste bissectrice (de rechte $x=y$ of ook nog de rechte onder hoek $\\alpha=\\frac{\\pi}{4}$ - altijd in radialen! ).  Het verband tussen de twee is\n",
    "$$O_{sp,2} = O_{rot,\\alpha}\\cdot O_{sp,1}\\cdot O_{rot,-\\alpha}\\,,$$\n",
    "of: \"spiegelen over rechte met hoek $\\alpha$ is gelijk aan roteren over $-\\alpha$, spiegelen rond de X-as en terug roteren over $\\alpha$\".\n",
    "\n",
    "Nu komen de rotaties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ce983eaf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Orot = np.array([[np.cos(a),-np.sin(a)],[np.sin(a),np.cos(a)]])\n",
    "vs=[np.array([1,0])]\n",
    "for i in range(7):\n",
    "    vs += [np.dot(Orot,vs[-1])]\n",
    "xs = [v[0] for v in vs]\n",
    "ys = [v[1] for v in vs]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(xs,ys)\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met de ingevoerde rotatie over hoek $\\alpha$ (remember $\\alpha=\\frac{\\pi}{4}$) genereert de rotatie inderdaad achtereenvolgens alle punten van een regelmatige achthoek. We kunnen ook testen of de rotatie leidt tot de identiteit tussen de spiegelingen\n",
    "$$O_{sp,2} = O_{rot,\\alpha}\\cdot O_{sp,1}\\cdot O_{rot,-\\alpha}\\,,$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.]\n",
      " [ 1. -0.]]\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Osp2.round(2))\n",
    "print(np.dot(np.dot(Orot,Osp1),Orot.T).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks out tot (bijna) machine precision. De redenering is onafhankelijk van de hoek $\\alpha$ die we gebruikt hebben. Voor de laatste factor hadden we de rotatie over de tegengestelde hoek nodig, en hebben we identiteit gebruikt:\n",
    "$$O_{rot,-\\alpha}=O_{rot,\\alpha}^{-1}=O_{rot,\\alpha}^{T}$$\n",
    "De rechtese identeit hebben we al ontmoet, als fundamentele eigenschap van isometrie\\\"en wat rotaties zeker zijn, en de rest kan expliciet gechecked worden rekening houdend met even/oneven karakter van cos/sin.\n",
    "\n",
    "\n",
    "#### 3D isometrieen\n",
    "\n",
    "Over spiegelingen in 3D kunnen we kort zijn: Een spiegeling door het XY-vlak wordt bepaald door"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0 -1]]\n"
     ]
    }
   ],
   "source": [
    "Ospxy = np.diag([1,1,-1])\n",
    "print(Ospxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en alle andere spiegelingen zijn geroteerde versies daarvan. Een spiegeling over achtereenvolgens XY, YZ en ZX vlak wordt zo gegeven door \n",
    "$$O_{sp3}=\\left[\\begin{array}{ccc}-1&0&0\\\\0&-1&0\\\\0&0&-1\\end{array}\\right]$$\n",
    "en is een combinatie van de spiegeling om een vlak en een rotatie van 180 graden rond zijn normaalvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotaties dan. Elke rotatie wordt gegeven door niet 1 maar 2 hoeken. Als we ons daarboven ook beperken tot rotaties over X, Y of Z as hebben we nog een extra hoek nodig. De individuele rotaties zijn dan identiek aan de 2D versie, maar met een extra rij en kolom in de matrix met een 1 op de diagonaal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rot3(angles):\n",
    "    [a,b,c]=angles\n",
    "    X = np.eye(3,3)\n",
    "    X[1:,1:]=np.array([[np.cos(a),-np.sin(a)],[np.sin(a),np.cos(a)]])\n",
    "    Z = np.eye(3,3)\n",
    "    Z[:2,:2]=np.array([[np.cos(c),-np.sin(c)],[np.sin(c),np.cos(c)]])\n",
    "    Y = np.eye(3,3)\n",
    "    Y[-1::-2,-1::-2] = np.array([[np.cos(b),-np.sin(b)],[np.sin(b),np.cos(b)]])\n",
    "    M = np.dot(Z,np.dot(Y,X))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor rotaties over 90 graden rond de verschillende assen vinden we bijvoorbeeld:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  0. -1.]\n",
      " [ 0.  1.  0.]]\n",
      "[ 1.  0.  0.]\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [-1.  0.  0.]]\n",
      "[ 0.  0. -1.]\n",
      "[[ 0. -1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "[ 0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "X=rot3([np.pi / 2,0,0])\n",
    "Y=rot3([0,np.pi / 2,0])\n",
    "Z=rot3([0,0,np.pi / 2])\n",
    "v=np.array([1,0,0])\n",
    "print(np.round(X,2))\n",
    "print(np.dot(X,v).round(2))\n",
    "print(np.round(Y,2))\n",
    "print(np.dot(Y,v).round(2))\n",
    "print(np.round(Z,2))\n",
    "print(np.dot(Z,v).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toepassing: cross product\n",
    "\n",
    "Een vereenvoudiging treedt op als we over kleine hoeken roteren. De rotatiematrix is dan ongeveer de identiteit plus een correctie lineair in die hoeken, terwijl in het algemeen geval het verband trigonometrisch en dus zeker niet lineair is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -0.03  0.02]\n",
      " [ 0.03  0.   -0.01]\n",
      " [-0.02  0.01  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "w=np.array([0.01,0.02,0.03])\n",
    "M=rot3(w)\n",
    "print(M.round(2)-np.eye(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laten we die kleine correctie -rotatie min de identiteit- inwerken op een vector, krijgen we een nieuwe vector. Op die manier definieren we een nieuwe bewerking linear in 2 vectoren die een 3e vector geeft. Unsurprisingly is dat het cross-product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01  0.02 -0.01]\n",
      "[-0.01  0.02 -0.01]\n"
     ]
    }
   ],
   "source": [
    "v=np.array([1,1,1])\n",
    "v2=np.dot(M-np.eye(3,3),v)\n",
    "v3=np.cross(w,v)\n",
    "print(v2.round(2))\n",
    "print(v3.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het is duidelijk dat het parallel stuk tussen rotatie en geroteerde vector geen invloed mag hebben en de correctie verdwijnt dus bij parallelle vectoren. Uit de matrixvoorstelling volgen ook alle andere cross-product eigenschappen: verwisselen van vectoren geeft een minteken, het resultaat staat loodrecht op de gegeven vectoren en de grootte van het resultaat is sinus van de ingesloten hoek. Dat is logisch omdat de parallelle componenten zoals gezegd niet bijdragen, en de loodrechte componenten bijdragen met een sinus-fractie. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexe geometrie\n",
    "\n",
    "We hebben gezien dat rotaties in 2D de structuur \n",
    "$$ O_{rot,\\alpha}=\\cos\\alpha\\,\\left[\\begin{array}{cc}1&0\\\\0&1\\end{array}\\right]+\\sin\\alpha\\,\\left[\\begin{array}{cc}0&-1\\\\1&0\\end{array}\\right]=\\cos\\alpha\\cdot 1+\\sin\\alpha\\cdot I$$\n",
    "hebben, waarbij de matrix $1$ alle eigenshcappen van het getal 1 heeft, maar de matrix $I$ is bijzonder. Als we ook $I$ konden identificeren met een soort getal werd rekenen met 2D rotaties een pak eenvoudiger, zonder matrices. Het enige het enige wat we nodig hebben voor zo'n \"scalaire\" voorstelling van de 2D matrices, zijn de producten\n",
    "$$ 1\\cdot 1=1\\,,1\\cdot I=I\\,, I\\cdot 1 = I\\,,I\\cdot I = -1$$\n",
    "Met die rekenregels kunnen we alle rotaties samenstellen, hoe ingewikkeld ook. We kunnen ook de vector $(x,y)$ voorstellen als (de eerste kolom van) $x+I\\cdot y$ en zo rotaties uitvoeren door de nieuw gedefinieerde productregels. Het resultaat geeft dan een nieuwe vector: (de eerste kolom van) $x'+I\\cdot y'$\n",
    "\n",
    "De nieuwe algebra waarbij we een getal hebben dat kwadrateert tot $-1$ heet de complexe getallen. Zoals je kan nagaan is de matrix $I$ een rotatie over 90 graden en tweemaal toegepast geeft die inderdaad het tegengestelde. Als de algebra los van matrices wordt gebruikt noteren we $I$ meestal als $i$. In principe laat de algebra ons toe in 2D te rekenen zonder matrices. Alle Complexe getallen komen dan overeen met een vector, terwijl alle unitaire complexe getallen (van de vorm $\\cos\\alpha+i\\,\\sin\\alpha$) overeen komen met rotaties.\n",
    "\n",
    "In 3D kan je iets gelijkaardig doen. Er bestaat een constructie (associatieve ring) met 3 getallen die kwadrateren tot $-1$:\n",
    "$$i^2=j^2=k^2=-1$$ \n",
    "zodat \n",
    "$$ij=k\\,,jk=i\\,,ki=j\\,,ji=-k\\,,kj=-i\\,,ik=-j\\,,$$\n",
    "Vaag komen i,j en k overeen met met weer een rotatie over 90 graden, maar niet helemaal. Getallen van de vorm\n",
    "$$a+b\\, i+c\\, j+d\\, k$$\n",
    "worden quaternionen genoemd en als we quaternionen van de vorm\n",
    "$$v_x\\,i+v_y\\,j+v_z\\,k$$identificeren met een vector $\\vec v$ terwijl we rotaties over $\\theta$ rond de vector $\\vec n$ voorstellen als unitaire quaternionen:\n",
    "$$\\cos\\frac{\\theta}{2}+\\sin\\frac{\\theta}{2}\\left(n_x\\,i+n_y\\,j+n_z\\,k\\right)$$\n",
    "reproduceren we de hele 3D rotatie-algebra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter3: Technicalities\n",
    "\n",
    "Bijna elke vierkante matrix kan worden voorgesteld in de vorm\n",
    "$$M=S\\cdot D\\cdot S^{-1}$$\n",
    "waarbij $D$ een diagonaalmatrix is. Voor symmetrische matrices is dat zelfs sterker:\n",
    "$$M=O\\cdot D\\cdot O^T$$\n",
    "met $O$ een orthoganale matrix, een rotatie dus. Dit kan geinterpreteerd worden als \"elke symmetrische matrix is een geroteerde versie van een uitrekking langs loodrechte assen\". De uitrekkingsfactoren zjn precies de elementen op de diagonaal van $D$. In het algemenere niet symmetrisch geval hoeven die assen dus niet loodrecht te zijn.\n",
    "\n",
    "De assen waarlangs de matrix $M$ een gewone uitrekking wordt worden eigenvectoren genoemd. Als voor 2 (of meer) assen dezelfde uitrekkingsfactor geldt, heet het opgespannen vlak een eigenruimte. De uitrekkingsfactoren zelf heten eigenvectoren. \n",
    "\n",
    "Het product van de eigenvectoren is de determinant. In tegenstelling tot de eigenvectoren en -waarden, kan de determinant bepaald worden door eenvoudige bewerkingen. Het eigenstelsel (waarden + vectoren) daarentegen vind je pas na het oplossen van vergelijkingen, en dat is aanzienlijk moeilijker. Dat betekent wel dat je kan te weten komen of een van de eigenwaarden 0 is (want dan is de determinant zeker ook 0 en omgekeerd) zonder het stelsel op te lossen.\n",
    "\n",
    "Als minstens een eigenwaarde 0 is, is de rank niet meer gelijk aan de dimensie waarin we werken. We hebben dat geillustreerd bij de projectie-toepassingen. Sommige vectoren zijn dan nooit een beeld van de matrix-afbeelding. Sommige vectoren worden ook op dezelfde vector afgebeeld, zodat informatie verloren gaat. Het is duidelijk dat de matrix dan niet inverteerbaar kan zijn, want die zou die het verloren onderscheid tussen vectoren met hetzelfde beeld moeten terugtoveren. Impossibru!\n",
    "\n",
    "In het geval dat geen eigenwaarden 0 zijn en dus de determinant niet 0 is, is de matrix altijd wel inverteerbaar en maximale rang. Dit is zo bij rotaties (determinant 1) spiegelingen (determiant -1) en meestal het geval dat we liefst hebben. \n",
    "\n",
    "## Determinant\n",
    "\n",
    "We spitsen ons enkel toe op 2D en 3D. De determinant bepalen is dan relatief eenvoudig:\n",
    "$$\\det\\left[\\begin{array}{cc}a&b\\\\c&d\\end{array}\\right] = ad-bc$$\n",
    "en \n",
    "$$\\det\\left[\\begin{array}{ccc}a&b&c\\\\d&e&f\\\\g&h&i\\end{array}\\right] = aei+dhc+bfg-(gec+dbi+ahf)$$\n",
    "Zoals gezegd: geen moeilijke bewerkingen en dit vertelt direct of het stelsel regulier ($det\\neq 0$) of singulier ($det = 0$) is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.72  0.   -0.96]\n",
      " [ 0.    2.    0.  ]\n",
      " [-0.96  0.    2.28]]\n",
      "6.0\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "-2.0\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[43,0,-24],[0,50,0],[-24,0,57]])/25\n",
    "print(A)\n",
    "print(np.linalg.det(A))\n",
    "\n",
    "B=np.arange(4).reshape((2,2))\n",
    "print(B)\n",
    "print(np.linalg.det(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenwaarden\n",
    "\n",
    "Het eigenstelsel van $M$ oplossen is het vinden van alle paren $(\\lambda_i,\\vec v_i)$ zodat $\\vec v_i$ de richting is waarin $M$ een eenvoudige herschaling wordt.\n",
    "$$M\\cdot\\vec v_i = \\lambda_i\\vec v_i$$\n",
    "Dat betekent dat voor elk zo'n $\\lambda_i$ de matrix $M-\\lambda\\, Id$ een $0$ eigenwaarde heeft, en dus \n",
    "$$\\det(M-\\lambda_i\\, Id)=0$$\n",
    "Omgekeerd zijn alle oplossingen van dat stelsel precies de eigenwaarden. Aangezien we de determinant gemakkelijk kunnen berekenen, is het vinden van die vergelijking eenvoudig. Oplossen is soms iets moeilijker met de hand, maar numeriek geen enkel probleem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  3.  2.]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.eigvals(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 43   0 -24]\n",
      " [  0  50   0]\n",
      " [-24   0  57]]\n"
     ]
    }
   ],
   "source": [
    "O=np.array([[4,0,3],[0,5,0],[3,0,-4]])\n",
    "D=np.diag([1,2,3])\n",
    "M=np.dot(O,np.dot(D,O.T))\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectoren\n",
    "\n",
    "Eens we de eigenwaarden $\\lambda_i$ gevonden hebben kunnen we voor elk de corresponderende eigenvector bepalen. Het oplossen van het stelsel\n",
    "$$\\det(M-\\lambda_i\\, Id)\\cdot\\vec v_i=0$$\n",
    "is speciaal omdat de matrix -per constructie- singulier is. Dat betekent dat het een soort projector is en dus dat meerdere vectoren op $0$ worden afgebeeld, die moeten we vinden. Er zijn veel truukjes voor, gezond verstand helpt ook. Een manier is het schrappen van een rij van de matrix en de deeldeterminanten van de resterende $1\\times 2$ of $2\\times 3$ matrix te gebruiken. Dat werkt \"meestal\": je wil dat de resterende matrix dan maximale rang heeft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96  0.    0.72]\n",
      "[ 0.  1.  0.]\n",
      "[-0.96  0.    1.28]\n"
     ]
    }
   ],
   "source": [
    "v = np.zeros((3,3))\n",
    "lda = [1,2,3]\n",
    "for i in range(3):\n",
    "    M = A - lda[i]*np.eye(3,3) \n",
    "    Mred=np.array([M[0], M[2]]).T if i==1 else M[:-1].T\n",
    "    v[i] = np.array([np.linalg.det(Mred[cols])  for cols in [[1,2],[2,0],[0,1]]])\n",
    "print(v[0].round(2))\n",
    "print(v[1].round(2))\n",
    "print(v[2].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vinden hetzelfde met ingebouwde functies, al zet die de eigenwaarden in een andere volgorde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  3.  2.]\n",
      "[[-0.8  0.  -0.6]\n",
      " [ 0.6  0.  -0.8]\n",
      " [ 0.   1.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.eig(A)[0])\n",
    "print(np.linalg.eig(A)[1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De vectoren geven enkel een richting aan een zijn dus bepaald op een voorfactor na. Meestal wordt die zo gekozen dat de vectoren genormalizeerd zijn. Dat laat ons toe er een isometrie uit te maken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    v[i] /= np.linalg.norm(v[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu is de cirkel rond, we hebben eigenvectoren gedefinieerd als krakteristieke herschalingsfactoren die ook in een diagonaalvorm van een matrix terug te vinden zijn. De eigenvecotren bepalen nu precies het geroteerde assenstelsel waar de matrix een herschaling is langs de assen en dus een diagonaalvorm heeft.\n",
    "\n",
    "De rotatie matrix $O$ is nu gemakkelijk te vinden, hij heeft de eigenvectoren als kolommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.72  0.   -0.96]\n",
      " [ 0.    2.    0.  ]\n",
      " [-0.96  0.    2.28]]\n"
     ]
    }
   ],
   "source": [
    "O=v.T\n",
    "D=np.diag([1,2,3])\n",
    "print(np.dot(np.dot(O,D),O.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We zien dat met de formule $$M=O\\cdot D\\cdot O^T$$ voor symmetrische matrices inderdaad is voldaan. Als de matrix niet symmetrisch was of de eigenvectoren niet genormaliseerd zijn, moeten we de inverse in plaats van de getransponeerde gebruiken.\n",
    "\n",
    "## Inverse\n",
    "\n",
    "We hebben al bij de toepassingen al gezien dat een inverse kan helpen om vergelijkigen op te lossen. We hebben toen ook gezegd dat de inverse van een diagonaalmatrix, de diagonaalmatrix van de inversen is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.        ]\n",
      " [ 0.          0.5         0.        ]\n",
      " [ 0.          0.          0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "Dinv = np.linalg.inv(D)\n",
    "print(Dinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je zou kunnen denken dat we nu gemakkelijk ook de inverse van $A$ kunnen berekenen, door eenvoudig dezelfde rotatie voor en na $D_{inv}$ te doen als voorheen. Dat zou volledig correct zijn:\n",
    "$$A^{-1} = O\\cdot D^{-1}\\cdot O^T$$\n",
    "Waar we voorheen $A$ roteerden naar het stelsel waar het een hershcaling is langs de assen, en dan terug, brengt dezelfde rotatie ons in een stelsel waar we de inverse herschaling kunnen doen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.76        0.          0.32      ]\n",
      " [ 0.          0.5         0.        ]\n",
      " [ 0.32        0.          0.57333333]]\n",
      "[[ 0.76        0.          0.32      ]\n",
      " [ 0.          0.5         0.        ]\n",
      " [ 0.32        0.          0.57333333]]\n"
     ]
    }
   ],
   "source": [
    "Ainv=np.dot(np.dot(O,Dinv),O.T)\n",
    "print(Ainv)\n",
    "print(np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het spreekt voor zich dat de determinant niet nul kan zijn als we willen inverteren, en we zien ook duidelijk dat in dat geval een $1/0$ verschijnt in de diagonaalmatrix. Er bestaat een formule om de inverse van een matrix te vinden zonder het eigenstelsel op te lossen, op dezelfde manier als we de determinant konden vinden zonder de eigenwaarden uit te rekenen. We gaan de algemene formule overslaan, maar geven wel het 2D en 3D geval:\n",
    "$$\\left[\\begin{array}{cc}a&b\\\\c&d\\end{array}\\right]^{-1}=\\frac{\\left[\\begin{array}{cc}d&-b\\\\-c&a\\end{array}\\right]}{ad-bc}$$\n",
    "and \n",
    "$$\\left[\\begin{array}{ccc}a&b&c\\\\d&e&f\\\\g&h&i\\end{array}\\right]^{-1} = \\frac{\\left[\\begin{array}{ccc}ei-hf&gf-di&dh-ge\\\\hc-bi&ai-gc&gb-ah\\\\bf-ec&dc-af&ae-db\\end{array}\\right]}{aei+dhc+bfg-(gec+dbi+ahf)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stelsel van Cramer\n",
    "\n",
    "Als we de inverse loslaten op een vector worden de bovenstaande formules eenvoudiger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
