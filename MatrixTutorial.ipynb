{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def printmat(M):\n",
    "    for i in range(len(M)):\n",
    "        print(M[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stefaans Grote Matrix-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Het concept\n",
    "\n",
    "##  Intro\n",
    "\n",
    "Er zijn veel manieren om matrices te introduceren. We volgen de meest intuitieve: onze matrices zijn lineare afbeeldingen tussen reele eindigdimensionele vectorruimtes. Say what?\n",
    "\n",
    "We zullen de introductie van matrices daarom doen in 2 stappen: we beschrijven eerst de vectorruimten waartussen ze werken, en dan de matrix-afbeeldingen zelf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorspace\n",
    "\n",
    "Een reele n-dimensionele vectorruimte is een moeilijke naam voor de verzameling van n-tupels met reele getallen als componenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=3\n",
    "vectorA = np.array( [1,1.3,3.1415926] )\n",
    "vectorB = np.array( [2,1.7,3.1415926] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "met als eigenschap dat we tupels kunnen optellen en vermenigvuldigen met reele getallen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorC = vectorA * 1.2 + vectorB * 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vec v_C$ is een $lineaire\\ combinatie$ van $\\vec v_A$ en $\\vec v_B$ en daarom ook een $n-D$ vector. So far so good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineaire afbeeldingen\n",
    "\n",
    "Een lineare afbeelding $F$ tussen vectorruimten maakt van elke n-dimensionele vector een m-dimensionale vector, bijvoorbeeld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 2\n",
    "def F(v):\n",
    "    return v[:2]\n",
    "wectorA = F(vectorA)\n",
    "wectorB = F(vectorB)\n",
    "wectorC = F(vectorC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zodat het beeld van lineare combinaties de lineaire combinatie van de beelden is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wectorC = wectorA * 1.2 + wectorB * 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kan nagaan dat dit effectief klopt voor $F$. Natuurlijk zijn er veel, veel meer van dergelijke functies. Volgens onze definitie worden ze allemaal voorgesteld door matrices, en ook omgekeerd geeft elke matrix zo'n functie. Why's that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e0 = np.array( [1,0,0] )\n",
    "e1 = np.array( [0,1,0] )\n",
    "e2 = np.array( [0,0,1] )\n",
    "\n",
    "vectorA = vectorA[0] * e0 + vectorA[1] * e1 + vectorA[2] * e2\n",
    "wectorA = vectorA[0] * F(e0) + vectorA[1] * F(e1) + vectorA[2] * F(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je ziet hierboven dat we $\\vec w_A$ gevonden hebben door slechts 3 beelden te berekenen: $F(\\vec e_1)$, $F(\\vec e_2)$ en $F(\\vec e_3)$. Met diezelfde 3 wordt $elk$ vectorbeeld gevonden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wectorB = vectorB[0] * F(e0) + vectorB[1] * F(e1) + vectorB[2] * F(e2)\n",
    "wectorC = vectorC[0] * F(e0) + vectorC[1] * F(e1) + vectorC[2] * F(e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F$ wordt dus volledig bepaald door de volgende gegevens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[0 1]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "MT = np.array( [F(e0), F(e1), F(e2)] )\n",
    "printmat(MT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waar eerst alle beelden $\\vec w_i$ uit $\\vec v_i$ werden gehaald met $F$, gebeurt dat nu met de getallen in $M_t$, die we een matrix noemen. Zoals gezegd, de matrix $M^T$ is equivalent aan een lineaire afbeelding $F$ tussen eindig dimensionele reele vectorruimten. Dat gaat op voor elke lineaire afbeelding $F$!\n",
    "\n",
    "De manier waarop we $M^T$ gebruiken is hierboven al aangegeven: elk beeld is een lineaire combinatie van de basis-beeldvectoren ${\\vec F} (\\vec e_0)$, $\\vec F (\\vec e_1)$ en $\\vec F (\\vec e_2)$\n",
    "$$ \\vec w_A = v_A[0]\\,\\vec F (\\vec e_0)+v_A[1]\\,\\vec F (\\vec e_1)+v_A[2]\\,\\vec F (\\vec e_2)$$\n",
    "en zo verder voor alle $\\vec w_i$'s. \n",
    "\n",
    "De standaard schrijfwijze voor zo'n lineaire combinatie --remember: niets meer dan een som van vectoren met elk eventueel een extra factor-- is als volgt:\n",
    "$$\\vec w_A = \\left[v_A[0]\\,,v_A[1]\\,,v_A[2]\\right]\\ \\cdot\\ \\left[\\begin{array}{c}\\vec F (\\vec e_0)\\\\\\vec F (\\vec e_1)\\\\\\vec F (\\vec e_2)\\end{array}\\right]=\\vec v_A\\ \\cdot\\ M^T$$\n",
    "en analoog voor andere vectoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wectorA = np.dot( vectorA,MT )\n",
    "wectorB = np.dot( vectorB,MT )\n",
    "wectorC = np.dot( vectorC,MT )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hebben dus het matrix-dotproduct ingevoerd, in dit geval om een vector met een matrix te vermenigvuldigen. Het resultaat is de som van alle rijen van de matrix met de vectorcomponenten als coefficienten\n",
    "$$ \\vec w_A = v_A[0]\\,\\vec F (\\vec e_0)+v_A[1]\\,\\vec F (\\vec e_1)+v_A[2]\\,\\vec F (\\vec e_2)$$\n",
    "zoals we al hadden gezien.\n",
    "\n",
    "Denk eraan dat de uitdrukking hierboven een gelijkheid is tussen 2-componentsvectoren. We zouden het resultaat ook kunnen schrijven voor beide componenten:\n",
    "$$ \\vec w_A[0] = v_A[0]\\, F (\\vec e_0)[0]+v_A[1]\\, F (\\vec e_1)[0]+v_A[2]\\, F (\\vec e_2)[0]$$\n",
    "en\n",
    "$$ \\vec w_A[1] = v_A[0]\\, F (\\vec e_0)[1]+v_A[1]\\, F (\\vec e_1)[1]+v_A[2]\\, F (\\vec e_2)[1]$$\n",
    "\n",
    "We gaan eens op een zotte quest en gaan 2 nieuwe vectoren maken uit de componenten van de $\\vec F(\\vec e_i)$'s of --equivalent-- uit de componenten van $M^T$.\n",
    "$$\\vec M_0=[ F (\\vec e_0)[0]\\,, F (\\vec e_1)[0]\\,, F (\\vec e_2)[0]]$$\n",
    "en\n",
    "$$\\vec M_1=[ F (\\vec e_0)[1]\\,, F (\\vec e_1)[1]\\,, F (\\vec e_2)[1]]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M0 = np.array( [ F(e0)[0],F(e1)[0],F(e2)[0] ] )\n",
    "M1 = np.array( [ F(e0)[1],F(e1)[1],F(e2)[1] ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met minimale Math-Sherlock-skills vinden we nu 2 connecties: Ten eerst zijn de vectoren hierboven niets anders dan de kolommen van $M^T$, as you can check. \n",
    "$$M^T=[\\vec M_0^T,\\vec M_1^T]$$\n",
    "Ten tweede worden de componenten die we hierboven hadden afgeleid, eenvoudig geschreven als\n",
    "$$\\vec w_A[0] = v_A[0]\\,M_0[0]+v_A[1]\\,M_0[1]+v_A[2]\\,M_0[2]=\\vec v_A\\cdot\\vec M_0$$\n",
    "$$\\vec w_A[1] = v_A[0]\\,M_1[0]+v_A[1]\\,M_1[1]+v_A[2]\\,M_1[2]=\\vec v_A\\cdot\\vec M_1$$\n",
    "of korter:\n",
    "$$\\vec w_A = [ \\vec v_A\\cdot\\vec M_0\\,, \\vec v_A\\cdot\\vec M_1]$$\n",
    "\n",
    "Alweer twee conclusies: Ten eerste heeft onze quest ons een nieuwe schijfwijze opgeleverd voor het beeld $\\vec w_A=\\vec F(\\vec v_A)$, nice! Dat beindigt in feite de inleiding tot matrices als lineaire afbeeldingen. We zijn klaar voor wat technische stuff. Ten tweede... wait, what? ... een dot-product tussen vectoren?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dots, dots, dots...\n",
    "\n",
    "Het is tijd om 'the talk' te hebben. Technical, notational mumbo-jumbo waar we door moeten. \n",
    "\n",
    "### Vector dot\n",
    "\n",
    "We hebben in feite enkel twee manieren gezien om een matrix --die een lineaire afbeelding voorstelt-- te dotten met een vector, wat ultiem hetzelfde was als de originele afbeelding zelf uitvoeren. Methode 1 zei dat het beeld een combinatie van de matrix-rijen was, met de componenten van het origineel als coefficient. Methode 2 zei dat de componenten van het beeld elk een (ander) dot product waren van het origineel met een andere vector, de respectievelijke kolommen van de matrix, as we checked! (Or did you?)\n",
    "\n",
    "Dat 'ander' dot-product is het bekende dot-product (ook wel $in-product$ of $scalair-product$ of $contractie$) van vectoren van gelijke lengte en de som van producten van overeenkomstige coefficienten.\n",
    "\n",
    "$$\\vec v_A\\cdot\\vec v_B = v_A[0]v_B[0]+v_A[1]v_B[1]+v_A[2]v_B[2]$$\n",
    "\n",
    "We gaan direct in op het verband tussen beide dot-producten. Ze zijn -natuurlijk- intrinsiek hetzelfde, maar enkel notationeel licht verschillend.\n",
    "\n",
    "### Duale dots\n",
    "\n",
    "Een van de matrix-eigenschappen die we vooropgesteld hebben is dat ze en afbeelding zijn van een $n$-D naar een $m$-D vectorruimte. Dat wordt ook duidelijk uit de structuur van de matrix $M^T$ die $n$ rijen heeft en $m$ kolommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[0 1]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "printmat(MT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een speciaal geval is $m=1$. De matrix (say $N^T$) bestaat dan uit een enkele kolom-vector,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.]\n",
      "[ 1.7]\n",
      "[ 3.1415926]\n"
     ]
    }
   ],
   "source": [
    "NT = np.array( [[2],[1.7],[3.1415926]] )\n",
    "printmat(NT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " en de afbeelding of dus ons matrix-product genereert een $1-D$ vector aka een getal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 14.07960406]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot( vectorA,NT ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hebben niet toevallig de componenten in $N^T$ gelijk gekozen aan die in $\\vec v_B$. Het laat ons toe het bovenstaande matrix-dot-product te vergelijken met een vector-dot-product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0796040644\n"
     ]
    }
   ],
   "source": [
    "print(np.dot( vectorA,vectorB ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, same thing. Het standaard vector-dot-product is niets anders dan het matrix-dot-product met een 1-kolom matrix aka $getransponeerde$ vector. \n",
    "\n",
    "Dat is een heel standaard ding: rij-vectoren, kolom-vectoren, matricesmet slechts 1 rij of kolom zijn allemaal volledig equivalent aan elkaar (de algebraische term is $isomorf$).\n",
    "\n",
    "Dat zorgt ervoor dat in veel codeertalen die dingen door elkaar worden gebruikt en bijvoorbeeld hetzelfde matrix-dot-product voor vectoren werkt (zoals hier in numpy). Word of caution, niet altijd wordt die sloppyness aanvaard, bijvoorbeeld niet in Matlab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of things\n",
    "\n",
    "Een gevolg van de verwisselbaarheid van rij en kolomvectoren is een soort mixed notation. Bear with me, dit is belangrijk. Meestal worden vectoren waarop een matrix inwerkt formeel als kolomvectoren geschreven. In code kunnen we de eenvoudiger rij-notatie gebruiken omdat het verschil -zoals gezegd- toch genegeerd wordt. Maar (!) het betekent dat de standaard volgorde bij de notatie van matrix-afbeeldingen omgekeerd is aan wat we tot hiertoe uit onze rij-vector notatie hebben afgeleid:\n",
    "$$ \\vec w_A = \\vec v_A\\cdot M^T\\quad\\Leftrightarrow \\vec w_A^T = (M^T)^T\\cdot\\vec v_A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De $T$ (transpose) switcht rij-en kolomvectoren en ook de overeenkomstige matrices. Twee keer switchen is natuurlijk hetzelfde als niets doen dus\n",
    "$$(M^T)^T=M$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n",
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "M = np.transpose(MT)\n",
    "M = MT.T\n",
    "printmat(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zo zien we hierboven dat inderdaad de matrix $M$ langs links inwerkt op de kolomvecotr $v_A^T$ terwijl voordien $M^T$ langs rechts op de rijvector $v_A$ inwerkte. Je hoeft het niet te lang te onthouden, het wordt compleet evident in de volgende sectie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   1.3]\n",
      "[ 1.   1.3]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot( MT.T,vectorA ))\n",
    "print(np.dot( vectorA,MT ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze nieuwe notatie wordt alles omgekeerd: We konden voordien de rijvector $\\vec w_A$ schrijven als hetzij een som van rijvectoren met de componenten $v_A[0]$m $v_A[1]$ en $v_A[2]$ als coefficienten:\n",
    "$$ \\vec w_A = v_A[0]\\,\\vec F (\\vec e_0)+v_A[1]\\,\\vec F (\\vec e_1)+v_A[2]\\,\\vec F (\\vec e_2)$$\n",
    "of -equivalent- als een rij-vector met vector dot-product componenten:\n",
    "$$\\vec w_A = [ \\vec v_A\\cdot\\vec M_0\\,, \\vec v_A\\cdot\\vec M_1]$$\n",
    "Check once again: same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   1.3]\n",
      "[1.0, 1.3]\n"
     ]
    }
   ],
   "source": [
    "print( vectorA[0]*F(e0)+vectorA[1]*F(e1)+vectorA[2]*F(e2)) \n",
    "print( [np.dot(vectorA,M0),np.dot(vectorA,M1)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omdat we nu een kolom-vector moeten uitkomen, zijn de 3 2-D vectoren $\\vec F(e_i)$ nu kolommen en de overeenkomstige matrix van de afbeelding heeft 2 rijen en 3 kolommen, ipv omgekeerd zoals voorheen. De conventie voor rij-vectoren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[0 1]\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "wectorA = np.dot( vectorA,MT )\n",
    "printmat(MT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordt nu dus vervangen door de kolom-vector notatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n",
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "wectorA = np.dot( M,vectorA )\n",
    "printmat(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "die een matrix met dezelfde informatie (want het is dezelfde afbeelding $F$) in een andere orde gebruikt. Vanaf nu werken transformatie-matrices in op vectoren langs de linkerkant, wat standaardnotatie is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains of dots\n",
    "\n",
    "Nu we alle mogelijke inwerking van matrices op vectoren begrijpen zijn we klaar om matrices met matrices te combineren. We zijn vertrokken met het idee dat matrix $M$ $n$-D vectoren omzet in $m$-D vectoren. Wat als we nu een andere matrix $K$ hebben die $m$-D vectoren omzet in $k$-D vectoren:\n",
    "$$\\vec w_A = M\\cdot \\vec v_A$$\n",
    "$$\\vec u_A = K\\cdot\\vec w_A$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "[1 0 0]\n",
      "[0 1 0]\n",
      "K\n",
      "[0 1]\n",
      "[2 3]\n",
      "[4 5]\n",
      "[6 7]\n"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "K = np.arange(k*m).reshape(k,m)\n",
    "print('M')\n",
    "printmat(M)\n",
    "print('K')\n",
    "printmat(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wat is dan de matrix die $\\vec v_A$ omzet in $\\vec u_A$? \n",
    "$$\\vec u_A = K\\cdot M\\cdot\\vec v_A\\stackrel{?}{=}L\\cdot \\vec v_A$$\n",
    "Blijkt dat $L$ bestaat en --niet onverwacht-- de combinatie van beelden van $M$-kolommen onder de linkse inwerking van $K$ of beelden van $K$-rijen onder de rechtse inwerking van $M$.\n",
    "$$L = \\left[\\begin{array}\\vec K[0]\\cdot M\\\\\\vec K[1]\\cdot M\\\\\\vec K[2]\\cdot M\\\\\\vec K[3]\\cdot M\\end{array}\\right]$$\n",
    "\n",
    "of\n",
    "$$L = [\\ K\\cdot \\vec M[:,0]\\ ,\\ K\\cdot \\vec M[:,1]\\ ,\\ K\\cdot \\vec M[:,2]\\ ]$$\n",
    "Twee equivalente berekeningswijzen die op hun beurt niets anders zeggen dan een 3e berekeningswijze:\n",
    "\n",
    "\"De $(i,j)$e component van $K\\cdot M$ is het vector-dot-product van de $i$e rij van $K$ met de $j$e kolom van $M$.\"\n",
    "\n",
    "en dit is de formele definitie van het matrixproduct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n",
      "[0 1 0]\n",
      "[2 3 0]\n",
      "[4 5 0]\n",
      "[6 7 0]\n"
     ]
    }
   ],
   "source": [
    "L = np.dot( K,M )\n",
    "#1e berekening\n",
    "L = np.array( [np.dot( K[i],M ) for i in range(k)] )\n",
    "#2e berekening\n",
    "L = np.array( [np.dot( K,M[:,j] ) for j in range(n)]).T\n",
    "#3e berekening\n",
    "L = np.array( [[np.dot( K[i],M[:,j] ) for j in range(n)] for i in range(k)] )\n",
    "print('L')\n",
    "printmat(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om matices te vermenigvuldigen moet dus het aantal kolommen (dimensie van de rij-vectoren) van de linkse gelijk zijn aan het aantal rijen(dimensie van de kolom-vectoren) van de rechtse. Om snel zo'n product uit te voeren nemen we denkbeeldig een ($i$e) rijvector van de linkermatrix, een geroteerde ($j$e) kolomvector van de rechter en leggen die over elkaar zodat we ze kunnen 'dotten'. Dat geeft ons een getal dat we in de overeenkomstige $(i,j)$e positie schrijven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecties en vierkante Matrices\n",
    "\n",
    "Om de inleiding te besluiten moeten we nog 2 belangrijke klasses van matrices bespreken. \n",
    "\n",
    "### Projecties\n",
    "\n",
    "Het kan gebeuren dat de 'uitgaande' kolomdimensie van een matrix, die in principe de dimensie vna de beelden bepaald, niet de echte dimensie van beeld is. Een voorbeeld is de projectie $P$ van een willekeurige $2D$ vector op een rechte, pak de eerste bissectrice $x=y$. De projectie rekt uit met het origineel, en een projectie van een som van vectoren is ook de som van projecties, alle voorwaarden voor een lineaire afbeelding en dus is er een matrixvoorstelling van de afbeelding. Hoewel de beelden een $x$- en $y$-component hebben, is het resultaat toch $1$-dimensionaal. \n",
    "\n",
    "Let's see. Om de projectie te maken hebben we een eenheidsvector nodig langs de $x=y$ lijn:\n",
    "$$\\vec e = [\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}]$$\n",
    "He is evident dat inderdaad $x=y$ en snel gechecked dat het een eenheidsvector is. De lengte van de projectie wordt nu gegeven door het dot-product:\n",
    "$$P(\\vec v)=\\vec e\\,(\\vec e\\cdot\\vec v)$$\n",
    "een uitdrukking die we als matrixproduct kunnen schrijven, het is snel gechecked dat de matrix\n",
    "$$P = \\left[\\begin{array}{cc}e[0]e[0]&e[0]e[1]\\\\e[1]e[0]&e[1]e[1]\\end{array}\\right]=\\left[\\begin{array}{cc}0.5&0.5\\\\0.5&0.5\\end{array}\\right]$$\n",
    "het voorgaande resultaat geeft. (We misbruiken vanaf nu wat notatie waarbij we dezelfde letter gebruiken voor matrix en afbeelding, anders wordt het alfabet snel te klein). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.15  1.15]\n",
      "[ 1.15  1.15]\n"
     ]
    }
   ],
   "source": [
    "wectorE = np.ones((2,)) / np.sqrt(2)\n",
    "print(wectorE * np.dot(wectorE,wectorA))\n",
    "print(np.dot( 0.5 * np.ones((2,2)),wectorA ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alweer vereist het geen echte Sherlock-skills om in te zien dat het beeld van een willekeurige vector onder P $x$ en $y$ component gelijk zal hebben, zoals bedoeld. Dat betekent omgekeerd dat een heel deel vectoren (die niet op de eerste bissectrice) nooit bereikt worden met een projectie. De eigenlijke dimensie van het beeld is dus 1, terwijl we er toch 2 componentsvectoren uit krijgen. De eigenlijke dimensie van het beeld wordt de $rang$ genoemd. \n",
    "\n",
    "### Vierkante matrices\n",
    "\n",
    "Het veruit meest interessante toepassingsdomein van matrices is het geval waarin ze afbeeldingen van en naar vectorruimten met dezelfde dimensie voorstellen. In dat geval hebben ze evenveel rijen als kolommen en worden ze \"vierkante matrices\" genoemd. De reden achter die naamgeving is zo complex... you would not understand.\n",
    "\n",
    "Een eerste extraatje bij vierkante matrices is dat ze oneindig dot-baar zijn, de restrictie dat er evenveel kolommen links als rijen rechts zijn in een matrixproduct vervalt natuurlijk als alle matrices evenveel rijen als kolommen hebben. \n",
    "\n",
    "Een 2e extraatje is dat ze soms inverteerbaar zijn. Niet transponeerbaar -dat zijn alle matrices- maar inverteerbaar. Het betekent dat elk origineel met juist een beeld correspondeert en we de omgekeerde bewerking kunnen maken. Het is duidelijk dat voor niet-vierkante matrices, als de dimensie van de beeld-ruimte verschilt van die van het origineel, zoiets onmogelijk is. Maar zelfs in het vierkante geval is inverteerbaarheid niet evident. Precies in het geval van projecties dat we net hebben besproken is dat niet mogelijk. Of anders gezegd: Een $n\\times n$ matrix is inverteerbaar als ook de rang gelijk is aan $n$.\n",
    "\n",
    "Is die inverse dan de getransponeerde? In het algemeen niet, maar soms wel. We zullen extra aandacht besteden aan de klasse van matrices waarvoor dit geldt -isometrieen- maar zeggen nog eens: zeker niet in het algemeen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Application time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan 3 soorten toepassingen bespreken. Matrices kunnen worden gebruikt om stelsels op te lossen in bijvoorbeeld vraagstukken, ze hebben een aantal statistische toepassingen zoals in Markov chains, lineaire regressie of zelfs neurale netwerken en tenslotte de klasse van isometrieen bij geometrische toepassingen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineaire Algebra\n",
    "\n",
    "\"Ik zie 10 koppen, 20 handen en 12 benen, hoeveel paarden, apen en kippen zijn er\" Is een klassiek vraagstuk. De vertaling naar een algebraisch stelsel gaat als volgt:\n",
    "$$\\begin{array}{rcl}10&=&p+a+m\\\\20 &=& 4\\,a\\\\12&=&4\\,p+2\\,k\\end{array}$$\n",
    "Wat met alle matrix-machinerie die we hebben ook kan geschreven worden als \n",
    "$$\\left[\\begin{array}{c}10\\\\20\\\\12\\end{array}\\right]=\\left[\\begin{array}{ccc}1&1&1\\\\0&4&2\\\\4&0&2\\end{array}\\right]\\cdot\\left[\\begin{array}{c}p\\\\a\\\\k\\end{array}\\right]$$\n",
    "om dit stelsel te kunnen oplossen moeten we de matrix inverteren, dat wil zeggen een matrix $A^{-1}$ vinden zodat\n",
    "$$A^{-1}\\cdot A=Id$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([[1,1,1],[4,0,0],[0,4,2]])\n",
    "telling = np.array([10,20,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om dat te kunnen moeten we eerst bepalen of de matrix $A$ geen projectie is. In dat geval is inverteren niet mogelijk, zoals we al hebben besproken. We hebben dan zowel beelden die geen origineel hebben als beelden die er veel hebben. Om te begrijpen wanneer en hoe we de inversie kunenn doen moeten we eerst een eenvoudiger probleem leren oplossen, een diagonaalmatrix inverteren.\n",
    "\n",
    "### Diagonaalmatrix\n",
    "\n",
    "Had ik in het vorige vraagstuk over mensen gehad en gevraagd naar het aantal neuzen, vingers en tenen, dan zag het stelsel er als volgt uit:\n",
    "$$\\left[\\begin{array}{c}10\\\\20\\\\12\\end{array}\\right]=\\left[\\begin{array}{ccc}1&0&0\\\\0&5&0\\\\0&0&5\\end{array}\\right]\\cdot\\left[\\begin{array}{c}n\\\\v\\\\t\\end{array}\\right]$$\n",
    "De matrix in de vergelijking is een diagonaalmatrix en gemakkelijk te inverteren: de inverse is eenvoudig die diagonaalmatrix van de inversen:\n",
    "$$\\left[\\begin{array}{ccc}1&0&0\\\\0&0.2&0\\\\0&0&0.2\\end{array}\\right]\\cdot\\left[\\begin{array}{c}10\\\\20\\\\12\\end{array}\\right]=\\left[\\begin{array}{c}n\\\\v\\\\t\\end{array}\\right]$$\n",
    "en het stelsel is snel opgelost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse\n",
      "[ 1.  0.  0.]\n",
      "[ 0.   0.2  0. ]\n",
      "[ 0.   0.   0.2]\n",
      "result\n",
      "[ 10.    4.    2.4]\n"
     ]
    }
   ],
   "source": [
    "B =np.diag([1,5,5])\n",
    "print('inverse')\n",
    "printmat(np.linalg.inv(B))\n",
    "print('result')\n",
    "print(np.dot(np.linalg.inv(B),telling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De voorwaarde voor inverteerbaarheid is hierbij duidelijk: geen van de diagonaalelementen van de matrix mocht $0$ zijn, en dus mag hun product niet nul zijn. Dat product heet de $determinant$ van de diagonaalmatrix. Het blijkt dat met bijna alle matrices een diagonaalmatrix overeenkomt, zodanig dat de inverse afhangt van de inverse van die diagonaalmatrix. Voor een willekeurige matrix is het product van de diagonaalelementen van de corresponderende diagonaalmatrix ook zijn determinant. De diagonaalelementen zelf worden eigenwaarden genoemd.\n",
    "\n",
    "Als de determinant 0 is, en de matrix heeft minstens een eigenwaarde die 0 word, dan is de rang niet meer maximaal en is de matrix een (mogelijk geschaalde) projectie. Het betekent dat een aantal componenten van het beeld in een vaste combinatie moeten voorkomen, zoals in ons projectievoorbeeld $x=y$. Voor al die combinaties zijn er oneindig veel originelen, en voor de andere zijn er geen. Dit is meestal niet ideaal om een stelsel op te lossen.\n",
    "\n",
    "Is de determinant verschillend van 0 is het stelsel wel oplosbaar. Dan kunnen we alle eigenwaarden en daardoor ook de matrix zelf inverteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverse\n",
      "[ 0.    0.25  0.  ]\n",
      "[-1.    0.25  0.5 ]\n",
      "[ 2.  -0.5 -0.5]\n",
      "result\n",
      "[ 5.  1.  4.]\n"
     ]
    }
   ],
   "source": [
    "print('inverse')\n",
    "printmat(np.linalg.inv(A))\n",
    "print('result')\n",
    "print(np.dot(np.linalg.inv(A),telling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De juiste berekening van eigenwaarden, determinanten en inversen zullen we op het einde van de tutorial nog eens uit de doeken doen. Dat is een beetje technisch.\n",
    "\n",
    "## Statistiek\n",
    "\n",
    "De meeste toepassingen van matrices gaan over vectorruimtes met een zekere symmetrie tussen de dimensies: lengte, hoogte en breedte zijn gelijkwaardige grootheden. Maar dat hoeft niet zo te zijn. \n",
    "\n",
    "Stel bijvoorbeeld dat we de afwijking (langs ($a^\\parallel$ en loodrecht $a^\\perp$) op de as startpositie - put) van een golfbals finale positie na een slag tov de afstand voor de slag ($\\Delta$), de ranking van de speler($R$) en de leeftijd van de speler ($L$). We meten dat een heel jaar lang en vinden honderd koppels \n",
    "$$(\\Delta_i,R_i,L_i) \\mapsto (a^\\parallel_i,a^\\perp_i)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
